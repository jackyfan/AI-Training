# 1. å¯¼å…¥é¡¹ç›®æ‰€éœ€è¦çš„ä¾èµ–åŒ…
# import asyncio
import os
# import json
# import gradio as gr
from dotenv import load_dotenv
# from langchain_mcp_adapters.client import MultiServerMCPClient
# from langgraph.prebuilt import create_react_agent
from langchain.chat_models import init_chat_model

import asyncio
import uuid
from typing import List, Dict
import gradio as gr
from gradio import ChatMessage
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.prebuilt import create_react_agent
from langgraph.types import Command

# 12306 MCP Clientï¼šhttps://mcp.api-inference.modelscope.net/93609f9190dd47/sse
# é«˜å¾·åœ°å›¾ MCP Clientï¼šhttps://mcp.api-inference.modelscope.net/78754deb009545/sse
# é«˜å¾·å®˜ç½‘ MCP Clientï¼šhttps://mcp.amap.com/mcp?key=20af20ebe8ad890e6e774fb5c17aeb6d
# ç½‘é¡µçˆ¬è™« MCP Clientï¼šhttps://mcp.api-inference.modelscope.net/79953b62c6d047/sse
# è´¨è°±æœç´¢ MCP Clientï¼šhttps://open.bigmodel.cn/api/mcp/web_search/sse?Authorization=75368c94471a4529879826b719f585a9.IBLSj9qB02NDzvft


# 2. é…ç½®MCPåœ°å€ä¿¡æ¯
# 2.1 é«˜å¾·åœ°å›¾
gaode_map_server_config = {
    "url": "https://mcp.amap.com/mcp?key=20af20ebe8ad890e6e774fb5c17aeb6d", # è¿æ¥mcpå·¥å…·çš„åœ°å€
    "transport": "streamable_http" # è¿æ¥mcpæ–¹å¼   -  SSE      Streamable HTTP
}

# 2.2 ç«è½¦ç¥¨mcp
my12306_server_config = {
    "url": "https://mcp.api-inference.modelscope.net/93609f9190dd47/sse",
    "transport": "sse"
}

# 2.3 ç½‘ç»œçˆ¬è™«mcp
fetch_mcp_server_config = {
    "url": "https://mcp.api-inference.modelscope.net/79953b62c6d047/sse",
    "transport": "sse"
}

# 2.4 ç½‘ç»œæœç´¢mcp[æ™ºè°±]
search_mcp_server_config = {
    "url": "https://open.bigmodel.cn/api/mcp/web_search/sse?Authorization=75368c94471a4529879826b719f585a9.IBLSj9qB02NDzvft",
    "transport": "sse"
}

# 3. å°†è¿™å‡ ä¸ªæœåŠ¡ï¼Œå°è£…åˆ°mcpå®¢æˆ·ç«¯
mcp_client = MultiServerMCPClient({
    "é«˜å¾·åœ°å›¾": gaode_map_server_config,
    "1206ç«è½¦ç¥¨": my12306_server_config,
    "çˆ¬è™«å·¥å…·": fetch_mcp_server_config,
    "æœç´¢å·¥å…·": search_mcp_server_config
})


load_dotenv()
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
if not DEEPSEEK_API_KEY:
    raise ValueError("âŒ è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® DEEPSEEK_API_KEY=ä½ çš„å¯†é’¥")

llm = init_chat_model("deepseek-chat", model_provider="deepseek")
agent_cache = None

# å¼€å‘æ™ºèƒ½ä½“
async def create_agent():
    # è·å–MCPçš„æ‰€æœ‰tools
    mcp_tools = await mcp_client.get_tools()

    # print(len(mcp_tools))  # æ‰“å°toolsçš„ä¸ªæ•° : 55ä¸ª
    # print(mcp_tools[-2:]) # æ‰“å°æœ€åä¸¤ä¸ªtools
    return create_react_agent(
        llm,
        tools=mcp_tools,
        prompt='ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå°½å¯èƒ½çš„è°ƒç”¨å·¥å…·å›ç­”ç”¨æˆ·é—®é¢˜',
        checkpointer=InMemorySaver()  # åˆ›å»ºä¸€ä¸ªå†…å­˜çš„ä¿å­˜å™¨ï¼š ä¿å­˜å¯¹è¯ä¸Šä¸‹æ–‡
    )


agent = asyncio.run(create_agent())
# é…ç½®å‚æ•°ï¼ŒåŒ…å«ä¼šè¯ID
config = {
    "configurable": {
        # æ£€æŸ¥ç‚¹ç”±session_idè®¿é—®
        "thread_id": str(uuid.uuid4()),
    }
}



# res = agent.invoke(input={'messages': [HumanMessage(content='ä½ å¥½ï¼')]}, config=config)
# print(res)

def add_message(chat_history, user_message):
    if user_message:
        chat_history.append({"role": "user", "content": user_message})
    return chat_history, gr.Textbox(value=None, interactive=False)


async def submit_messages(chat_history: List[Dict]):
    """æµå¼å¤„ç†æ¶ˆæ¯çš„æ ¸å¿ƒå‡½æ•°"""
    user_input = chat_history[-1]['content']
    current_state = agent.get_state(config)
    full_response = ""  # ç´¯ç§¯å®Œæ•´å“åº”
    tool_calls = []  # è®°å½•å·¥å…·è°ƒç”¨

    # å¤„ç†ä¸­æ–­æ¢å¤æˆ–æ­£å¸¸æ¶ˆæ¯
    inputs = Command(resume={'answer': user_input}) if current_state.next else {
        'messages': [HumanMessage(content=user_input)]}

    async for chunk in agent.astream(
            inputs,
            config,
            stream_mode=["messages", "updates"],  # åŒæ—¶ç›‘å¬æ¶ˆæ¯å’ŒçŠ¶æ€æ›´æ–°
    ):
        if 'messages' in chunk:
            for message in chunk[1]:
                # å¤„ç†AIæ¶ˆæ¯æµå¼è¾“å‡º
                if isinstance(message, AIMessage) and message.content:
                    full_response += message.content
                    # æ›´æ–°æœ€åä¸€æ¡æ¶ˆæ¯è€Œéè¿½åŠ 
                    if chat_history and isinstance(chat_history[-1], ChatMessage) and 'title' not in chat_history[-1].metadata:
                        chat_history[-1].content = full_response
                    else:
                        chat_history.append(ChatMessage(role="assistant", content=message.content))
                    yield chat_history

                # å¤„ç†å·¥å…·è°ƒç”¨æ¶ˆæ¯
                elif isinstance(message, ToolMessage):
                    tool_msg = f"ğŸ”§ è°ƒç”¨å·¥å…·: {message.name}\n{message.content}"
                    chat_history.append(ChatMessage(role="assistant", content=tool_msg,
                                        metadata={"title": f"ğŸ› ï¸ Used tool {message.name}"}))
                    yield chat_history


# åˆ›å»ºGradioç•Œé¢
with gr.Blocks(
        title='æˆ‘çš„æ™ºèƒ½å°ç§˜ä¹¦',
        theme=gr.themes.Soft(),
        css=".system {color: #666; font-style: italic;}"  # è‡ªå®šä¹‰ç³»ç»Ÿæ¶ˆæ¯æ ·å¼
) as demo:
    # èŠå¤©å†å²è®°å½•ç»„ä»¶
    chatbot = gr.Chatbot(
        type="messages",
        height=500,
        render_markdown=True,  # æ”¯æŒMarkdownæ ¼å¼
        line_breaks=False  # ç¦ç”¨è‡ªåŠ¨æ¢è¡Œç¬¦
    )

    # è¾“å…¥ç»„ä»¶
    chat_input = gr.Textbox(
        placeholder="è¯·è¾“å…¥æ‚¨çš„æ¶ˆæ¯...",
        label="ç”¨æˆ·è¾“å…¥",
        max_lines=5,
        container=False
    )

    # æ§åˆ¶æŒ‰é’®
    with gr.Row():
        submit_btn = gr.Button("å‘é€", variant="primary")
        clear_btn = gr.Button("æ¸…ç©ºå¯¹è¯")

    # æ¶ˆæ¯æäº¤å¤„ç†é“¾
    msg_handler = chat_input.submit(
        fn=add_message,
        inputs=[chatbot, chat_input],
        outputs=[chatbot, chat_input],
        queue=False
    ).then(
        fn=submit_messages,
        inputs=chatbot,
        outputs=chatbot,
        api_name="chat_stream"  # APIç«¯ç‚¹åç§°
    )

    # æŒ‰é’®ç‚¹å‡»å¤„ç†é“¾
    btn_handler = submit_btn.click(
        fn=add_message,
        inputs=[chatbot, chat_input],
        outputs=[chatbot, chat_input],
        queue=False
    ).then(
        fn=submit_messages,
        inputs=chatbot,
        outputs=chatbot
    )

    # æ¸…ç©ºå¯¹è¯
    clear_btn.click(
        fn=lambda: [],
        inputs=None,
        outputs=chatbot,
        queue=False
    )

    # é‡ç½®è¾“å…¥æ¡†çŠ¶æ€
    msg_handler.then(
        lambda: gr.Textbox(interactive=True),
        None,
        [chat_input]
    )
    btn_handler.then(
        lambda: gr.Textbox(interactive=True),
        None,
        [chat_input]
    )

if __name__ == '__main__':
    demo.launch()